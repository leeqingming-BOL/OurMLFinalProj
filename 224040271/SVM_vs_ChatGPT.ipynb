{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMCdAZpn8wCp"
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9u_YWnUW9FTI"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AktVFe5e9h2Q"
      },
      "source": [
        "# Data Pre-pocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glo9k5pV9p85",
        "outputId": "26669d53-11a3-412c-8536-68a992fdece0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: (120, 4)\n",
            "Test set size: (30, 4)\n"
          ]
        }
      ],
      "source": [
        "iris = load_iris()\n",
        "\n",
        "# Features and target\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Test set size:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT31sfzs815f"
      },
      "source": [
        "# Fine-tuned SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsexh0ko9Mbm",
        "outputId": "d13bb0e5-5924-4f6c-f4ed-1c1617a6cce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'C': 1, 'degree': 2, 'gamma': 0.1, 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "    'degree': [2, 3, 4]  # only used for 'poly' kernel\n",
        "}\n",
        "grid_search = GridSearchCV(SVC(probability=True), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR2QLRn2-CC_",
        "outputId": "11bdd883-1c49-4f79-d6d2-8ef5f4ee73f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ground Truth labels: [0 2 1 1 0 1 0 0 2 1 2 2 2 1 0 0 0 1 1 2 0 2 1 2 2 1 1 0 2 0]\n",
            "SVM Prediction:      [0 2 1 1 0 1 0 0 2 1 2 2 2 1 0 0 0 1 1 2 0 2 1 2 2 2 1 0 2 0]\n"
          ]
        }
      ],
      "source": [
        "# Train SVM model with best parameters\n",
        "svm_model = grid_search.best_estimator_\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "print(f'Ground Truth labels: {y_test}')\n",
        "print(f'SVM Prediction:      {y_pred}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZXBrGRL-28D",
        "outputId": "071b41f9-60b9-4d72-85fb-fed87d718bc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.90      0.95        10\n",
            "   virginica       0.91      1.00      0.95        10\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwOKbGUO87A3"
      },
      "source": [
        "# ChatGPT-4o-mini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "hfrCixDW_X0q"
      },
      "outputs": [],
      "source": [
        "# API_KEY = \"OpenAI API Key\"\n",
        "client = OpenAI(api_key=\"OPENAI_API_KEY\", base_url=\"https://api.openai.com/v1\")\n",
        "N = len(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0p83pMvowPv"
      },
      "source": [
        "## Zero-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OYD36wtNo0tt",
        "outputId": "3dbd597f-9c80-4a46-87ea-aa130317ccfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting Test Example 0\n",
            "Predicting Test Example 1\n",
            "Predicting Test Example 2\n",
            "Predicting Test Example 3\n",
            "Predicting Test Example 4\n",
            "Predicting Test Example 5\n",
            "Predicting Test Example 6\n",
            "Predicting Test Example 7\n",
            "Predicting Test Example 8\n",
            "Predicting Test Example 9\n",
            "Predicting Test Example 10\n",
            "Predicting Test Example 11\n",
            "Predicting Test Example 12\n",
            "Predicting Test Example 13\n",
            "Predicting Test Example 14\n",
            "Predicting Test Example 15\n",
            "Predicting Test Example 16\n",
            "Predicting Test Example 17\n",
            "Predicting Test Example 18\n",
            "Predicting Test Example 19\n",
            "Predicting Test Example 20\n",
            "Predicting Test Example 21\n",
            "Predicting Test Example 22\n",
            "Predicting Test Example 23\n",
            "Predicting Test Example 24\n",
            "Predicting Test Example 25\n",
            "Predicting Test Example 26\n",
            "Predicting Test Example 27\n",
            "Predicting Test Example 28\n",
            "Predicting Test Example 29\n"
          ]
        }
      ],
      "source": [
        "true_label = []\n",
        "pred_label = []\n",
        "prompt = \"\"\"The input contains 4 elements, which are the length and the width of the sepals and petals of an iris flower, in centimeters. Base on the combination of these four features, help me predict the Output value, i.e., the exact spicy of the iris flower(0 = setosa, 1 = versicolor, 2 = virginica).**\n",
        "    Your response should only contain the Output value in the format of #your prediction label#.\\n\"\"\"\n",
        "for n in range(N):\n",
        "    print(\"Predicting Test Example\", n)\n",
        "\n",
        "\n",
        "    # Here we construct the prompt for querying the LLM\n",
        "\n",
        "    s = f\"Input: \" + str(X_test[n]) + \"\\n\"\n",
        "\n",
        "    prompt = s + prompt\n",
        "    # print(prompt)\n",
        "    # Sometimes the LLM may not return our desired results. So, we try we try querying the LLM up to max_tries times. If still unsuccessful, we return a random label as prediction.\n",
        "    max_tries = 5\n",
        "    err_counter = 0\n",
        "    while err_counter < max_tries:\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[{\"role\": \"user\",\n",
        "                           \"content\": prompt},\n",
        "                          ],\n",
        "                temperature=0.7,  # controls randomness\n",
        "                max_tokens=150,   # controls response length\n",
        "            )\n",
        "            response = completion.choices[0].message.content\n",
        "            pred = int(response.replace(\"#\", \"\"))\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error encountered: {e}. Retrying...\")\n",
        "            err_counter += 1\n",
        "\n",
        "    if err_counter == max_tries:\n",
        "        # if still unsuccessful after \"max_tries\" tries, return a random label\n",
        "        print(\"max number of tries exceeded\")\n",
        "        pred = random.randint(0, 2)\n",
        "\n",
        "    true_label.append(y_test[n])\n",
        "    pred_label.append(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpSW2MCmpWhJ",
        "outputId": "0467f5c4-53ed-48cf-d58e-77e4a18e6761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groundtrugh labels:\n",
            "[0, 2, 1, 1, 0, 1, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 2, 0, 2, 1, 2, 2, 1, 1, 0, 2, 0]\n",
            "Predicted labels by ICL:\n",
            "[1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2]\n",
            "\n",
            "ICL Accuracy: 0.36666666666666664\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       0.00      0.00      0.00        10\n",
            "  versicolor       0.38      0.90      0.53        10\n",
            "   virginica       0.40      0.20      0.27        10\n",
            "\n",
            "    accuracy                           0.37        30\n",
            "   macro avg       0.26      0.37      0.27        30\n",
            "weighted avg       0.26      0.37      0.27        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the results\n",
        "print(\"Groundtrugh labels:\")\n",
        "print(list(map(int,true_label)))\n",
        "print(\"Predicted labels by ICL:\")\n",
        "print(pred_label)\n",
        "\n",
        "accuracy = accuracy_score(true_label, pred_label)\n",
        "print(\"\\nICL Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(true_label, pred_label, target_names=iris.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5qBw33t_SMe"
      },
      "source": [
        "## Few-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "afLPhDW8Cwo0",
        "outputId": "7f4b7a35-bdf6-4b35-dff8-42621b822d23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting Test Example 0\n",
            "Predicting Test Example 1\n",
            "Predicting Test Example 2\n",
            "Predicting Test Example 3\n",
            "Predicting Test Example 4\n",
            "Predicting Test Example 5\n",
            "Predicting Test Example 6\n",
            "Predicting Test Example 7\n",
            "Predicting Test Example 8\n",
            "Predicting Test Example 9\n",
            "Predicting Test Example 10\n",
            "Predicting Test Example 11\n",
            "Predicting Test Example 12\n",
            "Predicting Test Example 13\n",
            "Predicting Test Example 14\n",
            "Predicting Test Example 15\n",
            "Predicting Test Example 16\n",
            "Predicting Test Example 17\n",
            "Predicting Test Example 18\n",
            "Predicting Test Example 19\n",
            "Predicting Test Example 20\n",
            "Predicting Test Example 21\n",
            "Predicting Test Example 22\n",
            "Predicting Test Example 23\n",
            "Predicting Test Example 24\n",
            "Predicting Test Example 25\n",
            "Predicting Test Example 26\n",
            "Predicting Test Example 27\n",
            "Predicting Test Example 28\n",
            "Predicting Test Example 29\n"
          ]
        }
      ],
      "source": [
        "true_label = []\n",
        "pred_label = []\n",
        "for n in range(N):\n",
        "    print(\"Predicting Test Example\", n)\n",
        "\n",
        "\n",
        "    # Here we construct the prompt for querying the LLM\n",
        "    prompt = \"Help me predict the Output value for the last Input. Your response should only contain the Output value in the format of #Output value#.\\n\"\n",
        "\n",
        "    s = \"\"\n",
        "    for i in np.arange(len(X_train)):\n",
        "        s += f\"Input: {X_train[i]}, Output: {y_train[i]}\\n\"\n",
        "    s += f\"Input: \" + str(X_test[n]) + \", Output: \"\n",
        "\n",
        "    prompt += s\n",
        "    # print(prompt)\n",
        "    # Sometimes the LLM may not return our desired results. So, we try we try querying the LLM up to max_tries times. If still unsuccessful, we return a random label as prediction.\n",
        "    max_tries = 5\n",
        "    err_counter = 0\n",
        "    while err_counter < max_tries:\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[{\"role\": \"user\",\n",
        "                           \"content\": prompt},\n",
        "                          ],\n",
        "                temperature=0.7,  # controls randomness\n",
        "                max_tokens=150,   # controls response length\n",
        "            )\n",
        "            response = completion.choices[0].message.content\n",
        "            pred = int(response.replace(\"#\", \"\"))\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error encountered: {e}. Retrying...\")\n",
        "            err_counter += 1\n",
        "\n",
        "    if err_counter == max_tries:\n",
        "        # if still unsuccessful after \"max_tries\" tries, return a random label\n",
        "        print(\"max number of tries exceeded\")\n",
        "        pred = random.randint(0, 2)\n",
        "\n",
        "    true_label.append(y_test[n])\n",
        "    pred_label.append(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1iT5K-YAJyz",
        "outputId": "b57b7834-3baa-4414-9c0a-fe1ec931e398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groundtrugh labels:\n",
            "[0, 2, 1, 1, 0, 1, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 2, 0, 2, 1, 2, 2, 1, 1, 0, 2, 0]\n",
            "Predicted labels by ICL:\n",
            "[0, 1, 1, 1, 0, 2, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 1, 0, 2, 1, 2, 2, 1, 1, 0, 2, 0]\n",
            "\n",
            "ICL Accuracy: 0.9\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       0.82      0.90      0.86        10\n",
            "   virginica       0.89      0.80      0.84        10\n",
            "\n",
            "    accuracy                           0.90        30\n",
            "   macro avg       0.90      0.90      0.90        30\n",
            "weighted avg       0.90      0.90      0.90        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the results\n",
        "print(\"Groundtrugh labels:\")\n",
        "print(list(map(int,true_label)))\n",
        "print(\"Predicted labels by ICL:\")\n",
        "print(pred_label)\n",
        "\n",
        "accuracy = accuracy_score(true_label, pred_label)\n",
        "print(\"\\nICL Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(true_label, pred_label, target_names=iris.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnL-pvpQDzto"
      },
      "source": [
        "## Self-consistency CoT Prompting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "collapsed": true,
        "id": "AuX-xBNMFuoL"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"contains 4 features: the length and the width of the sepals and petals of an iris flower, in centimeters. Base on the combination of these four features, help me predict the Output value, i.e., the exact spicy of the iris flower(0 = setosa, 1 = versicolor, 2 = virginica).**\\n\\\n",
        "Your response should only contain the Output value in the format of #your prediction label#.\n",
        "**Examples**:\\n\"\"\"\n",
        "\n",
        "s = \"\"\n",
        "for i in np.arange(len(X_train)):\n",
        "  s += f\" Input: {X_train[i]}, Output: {y_train[i]}\\n\"\n",
        "\n",
        "problem_str = f\"**Problem: Input: \" + str(X_test[0]) + \", \"\n",
        "\n",
        "task_str = \"\"\"**Your tasks:**\n",
        "  - The given 120 examples can be considered as labeled training pairs, try to solve the problem as a 3-class classification task.\\n\"\"\"\n",
        "\n",
        "consistency = \"\"\"**Consistency Check:**\n",
        "  - Sample several reasoning paths. i.e., for each test example, sample several different reasoning paths or try different methods to predict the label.\n",
        "  - Compare the answers and select the most frequently occurring result.\\n\"\"\"\n",
        "\n",
        "answer = \"\"\"**Final Answer:**\n",
        "  - After verifying consistency across samples, conclude with the most consistent answer.\"\"\"\n",
        "\n",
        "# print(problem_str + prompt + s + task_str + consistency + answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb_IcfrjZK9X",
        "outputId": "52acf27b-b484-44c9-e9f5-978f4f5f9080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#2#\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# Testing, ignore this chunk\n",
        "# completion = client.chat.completions.create(\n",
        "#                 model=\"gpt-4o-mini\",\n",
        "#                 messages=[{\"role\": \"user\",\n",
        "#                            \"content\": problem_str + prompt + s + task_str + reasoning + answer},\n",
        "#                           ],\n",
        "#                 temperature=0.7,  # controls randomness\n",
        "#                 max_tokens=150,   # controls response length\n",
        "#             )\n",
        "# response = completion.choices[0].message.content\n",
        "# print(response)\n",
        "# pred = int(response.replace(\"#\", \"\"))\n",
        "# print(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ymOsW8OYD9aF",
        "outputId": "1b924690-913f-49f4-84dc-f3f6c9adc69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting Test Example 0\n",
            "Predicting Test Example 1\n",
            "Predicting Test Example 2\n",
            "Predicting Test Example 3\n",
            "Predicting Test Example 4\n",
            "Predicting Test Example 5\n",
            "Predicting Test Example 6\n",
            "Predicting Test Example 7\n",
            "Predicting Test Example 8\n",
            "Predicting Test Example 9\n",
            "Predicting Test Example 10\n",
            "Predicting Test Example 11\n",
            "Predicting Test Example 12\n",
            "Predicting Test Example 13\n",
            "Predicting Test Example 14\n",
            "Predicting Test Example 15\n",
            "Predicting Test Example 16\n",
            "Predicting Test Example 17\n",
            "Predicting Test Example 18\n",
            "Predicting Test Example 19\n",
            "Predicting Test Example 20\n",
            "Predicting Test Example 21\n",
            "Predicting Test Example 22\n",
            "Predicting Test Example 23\n",
            "Predicting Test Example 24\n",
            "Predicting Test Example 25\n",
            "Predicting Test Example 26\n",
            "Predicting Test Example 27\n",
            "Predicting Test Example 28\n",
            "Predicting Test Example 29\n"
          ]
        }
      ],
      "source": [
        "true_label = []\n",
        "pred_label = []\n",
        "\n",
        "for n in range(N):\n",
        "    print(\"Predicting Test Example\", n)\n",
        "\n",
        "\n",
        "    # Here we construct the prompt for querying the LLM\n",
        "    problem_str = f\"**Problem: Input: \" + str(X_test[n]) + \", \"\n",
        "    prompt = problem_str + prompt\n",
        "    prompt += s\n",
        "    prompt += task_str\n",
        "    prompt += reasoning\n",
        "    prompt += answer\n",
        "    # print(prompt)\n",
        "    # Sometimes the LLM may not return our desired results. So, we try we try querying the LLM up to max_tries times. If still unsuccessful, we return a random label as prediction.\n",
        "    max_tries = 5\n",
        "    err_counter = 0\n",
        "    while err_counter < max_tries:\n",
        "        try:\n",
        "          completion = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[{\"role\": \"user\",\n",
        "                           \"content\": prompt},\n",
        "                          ],\n",
        "                temperature=0.7,  # controls randomness\n",
        "                max_tokens=150,   # controls response length\n",
        "            )\n",
        "          response = completion.choices[0].message.content\n",
        "          pred = int(response.replace(\"#\", \"\"))\n",
        "          break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error encountered: {e}. Retrying...\")\n",
        "            err_counter += 1\n",
        "\n",
        "    if err_counter == max_tries:\n",
        "        # if still unsuccessful after \"max_tries\" tries, return a random label\n",
        "        print(\"max number of tries exceeded\")\n",
        "        pred = random.randint(0, 2)\n",
        "\n",
        "    true_label.append(y_test[n])\n",
        "    pred_label.append(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oagxrFrBal6D",
        "outputId": "697eefb2-5f6a-43eb-f916-9e6652684e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groundtrugh labels:\n",
            "[0, 2, 1, 1, 0, 1, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 2, 0, 2, 1, 2, 2, 1, 1, 0, 2, 0]\n",
            "Predicted labels by ICL:\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "ICL Accuracy: 0.3\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       0.35      0.90      0.50        10\n",
            "  versicolor       0.00      0.00      0.00        10\n",
            "   virginica       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           0.30        30\n",
            "   macro avg       0.12      0.30      0.17        30\n",
            "weighted avg       0.12      0.30      0.17        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the results\n",
        "print(\"Groundtrugh labels:\")\n",
        "print(list(map(int,true_label)))\n",
        "print(\"Predicted labels by ICL:\")\n",
        "print(pred_label)\n",
        "\n",
        "accuracy = accuracy_score(true_label, pred_label)\n",
        "print(\"\\nICL Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(true_label, pred_label, target_names=iris.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG8-ChZSrmSD"
      },
      "source": [
        "# ChatGPT-4o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iziyfjTnr63G",
        "outputId": "b90092cf-1c10-42fe-eea7-f65721d17fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting Test Example 0\n",
            "Predicting Test Example 1\n",
            "Predicting Test Example 2\n",
            "Predicting Test Example 3\n",
            "Predicting Test Example 4\n",
            "Predicting Test Example 5\n",
            "Predicting Test Example 6\n",
            "Predicting Test Example 7\n",
            "Predicting Test Example 8\n",
            "Predicting Test Example 9\n",
            "Predicting Test Example 10\n",
            "Predicting Test Example 11\n",
            "Predicting Test Example 12\n",
            "Predicting Test Example 13\n",
            "Predicting Test Example 14\n",
            "Predicting Test Example 15\n",
            "Predicting Test Example 16\n",
            "Predicting Test Example 17\n",
            "Predicting Test Example 18\n",
            "Predicting Test Example 19\n",
            "Predicting Test Example 20\n",
            "Predicting Test Example 21\n",
            "Predicting Test Example 22\n",
            "Predicting Test Example 23\n",
            "Predicting Test Example 24\n",
            "Predicting Test Example 25\n",
            "Predicting Test Example 26\n",
            "Predicting Test Example 27\n",
            "Predicting Test Example 28\n",
            "Predicting Test Example 29\n"
          ]
        }
      ],
      "source": [
        "true_label = []\n",
        "pred_label = []\n",
        "for n in range(N):\n",
        "    print(\"Predicting Test Example\", n)\n",
        "\n",
        "\n",
        "    # Here we construct the prompt for querying the LLM\n",
        "    prompt = \"Help me predict the Output value for the last Input. Your response should only contain the Output value in the format of #Output value#.\\n\"\n",
        "\n",
        "    s = \"\"\n",
        "    for i in np.arange(len(X_train)):\n",
        "        s += f\"Input: {X_train[i]}, Output: {y_train[i]}\\n\"\n",
        "    s += f\"Input: \" + str(X_test[n]) + \", Output: \"\n",
        "\n",
        "    prompt += s\n",
        "    # print(prompt)\n",
        "    # Sometimes the LLM may not return our desired results. So, we try we try querying the LLM up to max_tries times. If still unsuccessful, we return a random label as prediction.\n",
        "    max_tries = 5\n",
        "    err_counter = 0\n",
        "    while err_counter < max_tries:\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[{\"role\": \"user\",\n",
        "                           \"content\": prompt},\n",
        "                          ],\n",
        "                temperature=0.7,  # controls randomness\n",
        "                max_tokens=150,   # controls response length\n",
        "            )\n",
        "            response = completion.choices[0].message.content\n",
        "            pred = int(response.replace(\"#\", \"\"))\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error encountered: {e}. Retrying...\")\n",
        "            err_counter += 1\n",
        "\n",
        "    if err_counter == max_tries:\n",
        "        # if still unsuccessful after \"max_tries\" tries, return a random label\n",
        "        print(\"max number of tries exceeded\")\n",
        "        pred = random.randint(0, 2)\n",
        "\n",
        "    true_label.append(y_test[n])\n",
        "    pred_label.append(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cokcH4EuwQxj",
        "outputId": "36555c1a-74d6-4bd0-c277-dfff09e9c831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Groundtrugh labels:\n",
            "[0, 2, 1, 1, 0, 1, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 2, 0, 2, 1, 2, 2, 1, 1, 0, 2, 0]\n",
            "Predicted labels by ICL:\n",
            "[0, 2, 1, 1, 0, 1, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 2, 0, 2, 1, 2, 2, 2, 1, 0, 2, 0]\n",
            "\n",
            "ICL Accuracy: 0.9666666666666667\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.90      0.95        10\n",
            "   virginica       0.91      1.00      0.95        10\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the results\n",
        "print(\"Groundtrugh labels:\")\n",
        "print(list(map(int,true_label)))\n",
        "print(\"Predicted labels by ICL:\")\n",
        "print(pred_label)\n",
        "\n",
        "accuracy = accuracy_score(true_label, pred_label)\n",
        "print(\"\\nICL Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(true_label, pred_label, target_names=iris.target_names))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
